{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/h/hhamidi/storage/miniconda3/envs/ldm/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torchvision import models\n",
    "from torch import nn\n",
    "from torch.nn import init\n",
    "\n",
    "class DensNetWithHead(nn.Module):\n",
    "    def __init__(self,  hidden_layer_sizes, dropout_rate, num_classes):\n",
    "        super(DensNetWithHead, self).__init__()\n",
    "\n",
    "        # Pretrained DenseNet backbone\n",
    "        self.backbone = models.densenet121(pretrained=True)\n",
    "        num_features = self.backbone.classifier.in_features\n",
    "\n",
    "        # Remove the last classification layer of the backbone\n",
    "        self.backbone.classifier = nn.Identity()\n",
    "\n",
    "        # Custom head with hidden layers\n",
    "        layers = []\n",
    "        input_size = num_features\n",
    "\n",
    "        for size in hidden_layer_sizes:\n",
    "            linear_layer = nn.Linear(input_size, size)\n",
    "            init.kaiming_uniform_(linear_layer.weight, nonlinearity='relu')\n",
    "            layers.append(linear_layer)\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.BatchNorm1d(size))\n",
    "            layers.append(nn.Dropout(dropout_rate))\n",
    "            input_size = size\n",
    "\n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(input_size, num_classes))\n",
    "\n",
    "        # Assemble the custom head\n",
    "        self.custom_head = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through the backbone\n",
    "        features = self.backbone(x)\n",
    "  \n",
    "\n",
    "        # Forward pass through the custom head\n",
    "        output = self.custom_head(features)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.image.dataset_class import Chexpert\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "\n",
    "# create the dataset\n",
    "\n",
    "dataset = Chexpert(dataframe_path = '/fs01/home/hhamidi/projects/stable-diffusion/data/csv_files/val_from_train.csv',\n",
    "                    path_image = '/datasets/chexpert/',\n",
    "                    MAPPING = {-1: 1, 0: 0, 1: 1, float('NaN'): 0},\n",
    "                    transform = transforms.Compose([\n",
    "                        transforms.Resize([224,224]),\n",
    "                        transforms.ToTensor(),\n",
    "                        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "                    ]))\n",
    "\n",
    "# if dataset.dataset_size > 2000:\n",
    "#     dataset.dataset_size = 2000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLS():\n",
    "    def __init__(self, model, device):\n",
    "        self.model = model\n",
    "        self.model.eval()\n",
    "        self.device = device\n",
    "        self.LABEL_NAMES =['Atelectasis',\n",
    "                            'Cardiomegaly',\n",
    "                            'Consolidation',\n",
    "                            'Edema',\n",
    "                            'Enlarged Cardiomediastinum',\n",
    "                            'Fracture',\n",
    "                            'Lung Lesion',\n",
    "                            'Lung Opacity',\n",
    "                            'No Finding',\n",
    "                            'Pleural Effusion',\n",
    "                            'Pleural Other',\n",
    "                            'Pneumonia',\n",
    "                            'Pneumothorax',\n",
    "                            'Support Devices']\n",
    "    def predict(self, image):\n",
    "        image = image.to(self.device)\n",
    "        with torch.no_grad():\n",
    "            if len(image.shape) == 3:\n",
    "                output = self.model(image[None, ...])\n",
    "\n",
    "                \n",
    "        output = torch.sigmoid(output)\n",
    "        result = dict(zip(self.LABEL_NAMES, output[0].tolist()))\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load('/fs01/home/hhamidi/fairness_on_embeddings/results/image_chexpert_real_real/checkpoints/best_val_loss-epoch=09-val_loss=0.2735.ckpt')['state_dict']\n",
    "\n",
    "new_state_dict = {}\n",
    "for k, v in state_dict.items():\n",
    "    name = k.replace('model.', '')\n",
    "    new_state_dict[name] = v\n",
    "\n",
    "\n",
    "model = DensNetWithHead(hidden_layer_sizes=[768, 128], dropout_rate=0.0, num_classes=14)\n",
    "model.load_state_dict(new_state_dict)\n",
    "model = model.to('cuda')\n",
    "cls = CLS(model, 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 44683/44683 [25:13<00:00, 29.51it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "# run the model on the dataset\n",
    "probabilities = []\n",
    "labels = []\n",
    "\n",
    "for i in tqdm(range(dataset.dataset_size)):\n",
    "    item =  dataset[i]\n",
    "    image, label = item['data'], item['labels']\n",
    "    labels.append(label.numpy())\n",
    "    output = cls.predict(image)\n",
    "    probabilities.append(list(output.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thersholds:  {'Atelectasis': 0.07908083498477936, 'Cardiomegaly': 0.2774554491043091, 'Consolidation': 0.05144466832280159, 'Edema': 0.25417467951774597, 'Enlarged Cardiomediastinum': 0.0847725048661232, 'Fracture': 0.2180563360452652, 'Lung Lesion': 0.12561748921871185, 'Lung Opacity': 0.32183897495269775, 'No Finding': 0.1821669042110443, 'Pleural Effusion': 0.3480605185031891, 'Pleural Other': 0.039286863058805466, 'Pneumonia': 0.030648918822407722, 'Pneumothorax': 0.15165171027183533, 'Support Devices': 0.42294201254844666}\n",
      "roc_auc:  {'Atelectasis': 0.6493029846273183, 'Cardiomegaly': 0.8603195599553128, 'Consolidation': 0.6320110751685948, 'Edema': 0.8146588978630822, 'Enlarged Cardiomediastinum': 0.6662043129304929, 'Fracture': 0.7999110742590022, 'Lung Lesion': 0.7832175786053984, 'Lung Opacity': 0.7078548857145512, 'No Finding': 0.8542649358299658, 'Pleural Effusion': 0.8611295308907205, 'Pleural Other': 0.774145772245826, 'Pneumonia': 0.7406809385422766, 'Pneumothorax': 0.8271683163795367, 'Support Devices': 0.8883832390143751}\n"
     ]
    }
   ],
   "source": [
    "# calculate the roc auc\n",
    "from metrics.metrics import find_best_threshold, calculate_roc_auc\n",
    "import numpy as np\n",
    "thersholds = find_best_threshold( np.array(probabilities),np.array(labels))\n",
    "# create a dictionary of the thresholds with CLS.LABEL_NAMES\n",
    "thersholds = dict(zip(cls.LABEL_NAMES, list(thersholds.values())))\n",
    "print('thersholds: ', thersholds)\n",
    "_,_,roc_auc = calculate_roc_auc(np.array(probabilities),np.array(labels))\n",
    "roc_auc = dict(zip(cls.LABEL_NAMES, list(roc_auc.values())))\n",
    "print('roc_auc: ', roc_auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upsampling Cardiomegaly...\n",
      "Upsampling Consolidation...\n",
      "Multi-label mode: True, Number of classes: [5]\n",
      "------------------------------\n",
      "Found 227395 images in total, 48021 positive images, 179374 negative images\n",
      "Cardiomegaly(C0): imbalance ratio is 0.2112\n",
      "\n",
      "Found 227395 images in total, 77866 positive images, 149529 negative images\n",
      "Edema(C1): imbalance ratio is 0.3424\n",
      "\n",
      "Found 227395 images in total, 27217 positive images, 200178 negative images\n",
      "Consolidation(C2): imbalance ratio is 0.1197\n",
      "\n",
      "Found 227395 images in total, 70593 positive images, 156802 negative images\n",
      "Atelectasis(C3): imbalance ratio is 0.3104\n",
      "\n",
      "Found 227395 images in total, 94036 positive images, 133359 negative images\n",
      "Pleural Effusion(C4): imbalance ratio is 0.4135\n",
      "\n",
      "------------------------------\n",
      "Multi-label mode: True, Number of classes: [5]\n",
      "------------------------------\n",
      "Found 202 images in total, 66 positive images, 136 negative images\n",
      "Cardiomegaly(C0): imbalance ratio is 0.3267\n",
      "\n",
      "Found 202 images in total, 42 positive images, 160 negative images\n",
      "Edema(C1): imbalance ratio is 0.2079\n",
      "\n",
      "Found 202 images in total, 32 positive images, 170 negative images\n",
      "Consolidation(C2): imbalance ratio is 0.1584\n",
      "\n",
      "Found 202 images in total, 75 positive images, 127 negative images\n",
      "Atelectasis(C3): imbalance ratio is 0.3713\n",
      "\n",
      "Found 202 images in total, 64 positive images, 138 negative images\n",
      "Pleural Effusion(C4): imbalance ratio is 0.3168\n",
      "\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as tfs\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "\n",
    "class CheXpert(Dataset):\n",
    "    '''\n",
    "    Reference: \n",
    "        @inproceedings{yuan2021robust,\n",
    "            title={Large-scale Robust Deep AUC Maximization: A New Surrogate Loss and Empirical Studies on Medical Image Classification},\n",
    "            author={Yuan, Zhuoning and Yan, Yan and Sonka, Milan and Yang, Tianbao},\n",
    "            booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},\n",
    "            year={2021}\n",
    "            }\n",
    "    '''\n",
    "    def __init__(self, \n",
    "                 csv_path, \n",
    "                 image_root_path='',\n",
    "                 image_size=320,\n",
    "                 class_index=0, \n",
    "                 use_frontal=True,\n",
    "                 use_upsampling=True,\n",
    "                 flip_label=False,\n",
    "                 shuffle=False,\n",
    "                 seed=123,\n",
    "                 verbose=True,\n",
    "                 upsampling_cols=['Cardiomegaly', 'Consolidation'],\n",
    "                 train_cols=['Cardiomegaly', 'Edema', 'Consolidation', 'Atelectasis',  'Pleural Effusion'],\n",
    "                 mode='train'):\n",
    "        \n",
    "    \n",
    "        # load data from csv\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.df['Path'] = self.df['Path'].str.replace('CheXpert-v1.0-small/', '')\n",
    "        self.df['Path'] = self.df['Path'].str.replace('CheXpert-v1.0/', '')\n",
    "        if use_frontal:\n",
    "            self.df = self.df[self.df['Frontal/Lateral'] == 'Frontal']  \n",
    "            \n",
    "        # upsample selected cols\n",
    "        if use_upsampling:\n",
    "            assert isinstance(upsampling_cols, list), 'Input should be list!'\n",
    "            sampled_df_list = []\n",
    "            for col in upsampling_cols:\n",
    "                print ('Upsampling %s...'%col)\n",
    "                sampled_df_list.append(self.df[self.df[col] == 1])\n",
    "            self.df = pd.concat([self.df] + sampled_df_list, axis=0)\n",
    "\n",
    "\n",
    "        # impute missing values \n",
    "        for col in train_cols:\n",
    "            if col in ['Edema', 'Atelectasis']:\n",
    "                self.df[col].replace(-1, 1, inplace=True)  \n",
    "                self.df[col].fillna(0, inplace=True) \n",
    "            elif col in ['Cardiomegaly','Consolidation',  'Pleural Effusion']:\n",
    "                self.df[col].replace(-1, 0, inplace=True) \n",
    "                self.df[col].fillna(0, inplace=True)\n",
    "            else:\n",
    "                self.df[col].fillna(0, inplace=True)\n",
    "        \n",
    "        self._num_images = len(self.df)\n",
    "        \n",
    "        # 0 --> -1\n",
    "        if flip_label and class_index != -1: # In multi-class mode we disable this option!\n",
    "            self.df.replace(0, -1, inplace=True)   \n",
    "            \n",
    "        # shuffle data\n",
    "        if shuffle:\n",
    "            data_index = list(range(self._num_images))\n",
    "            np.random.seed(seed)\n",
    "            np.random.shuffle(data_index)\n",
    "            self.df = self.df.iloc[data_index]\n",
    "        \n",
    "        \n",
    "        assert class_index in [-1, 0, 1, 2, 3, 4], 'Out of selection!'\n",
    "        assert image_root_path != '', 'You need to pass the correct location for the dataset!'\n",
    "\n",
    "        if class_index == -1: # 5 classes\n",
    "            print ('Multi-label mode: True, Number of classes: [%d]'%len(train_cols))\n",
    "            self.select_cols = train_cols\n",
    "            self.value_counts_dict = {}\n",
    "            for class_key, select_col in enumerate(train_cols):\n",
    "                class_value_counts_dict = self.df[select_col].value_counts().to_dict()\n",
    "                self.value_counts_dict[class_key] = class_value_counts_dict\n",
    "        else:       # 1 class\n",
    "            self.select_cols = [train_cols[class_index]]  # this var determines the number of classes\n",
    "            self.value_counts_dict = self.df[self.select_cols[0]].value_counts().to_dict()\n",
    "        \n",
    "        self.mode = mode\n",
    "        self.class_index = class_index\n",
    "        self.image_size = image_size\n",
    "        \n",
    "        self._images_list =  [image_root_path+path for path in self.df['Path'].tolist()]\n",
    "        if class_index != -1:\n",
    "            self._labels_list = self.df[train_cols].values[:, class_index].tolist()\n",
    "        else:\n",
    "            self._labels_list = self.df[train_cols].values.tolist()\n",
    "    \n",
    "        if verbose:\n",
    "            if class_index != -1:\n",
    "                print ('-'*30)\n",
    "                if flip_label:\n",
    "                    self.imratio = self.value_counts_dict[1]/(self.value_counts_dict[-1]+self.value_counts_dict[1])\n",
    "                    print('Found %s images in total, %s positive images, %s negative images'%(self._num_images, self.value_counts_dict[1], self.value_counts_dict[-1] ))\n",
    "                    print ('%s(C%s): imbalance ratio is %.4f'%(self.select_cols[0], class_index, self.imratio ))\n",
    "                else:\n",
    "                    self.imratio = self.value_counts_dict[1]/(self.value_counts_dict[0]+self.value_counts_dict[1])\n",
    "                    print('Found %s images in total, %s positive images, %s negative images'%(self._num_images, self.value_counts_dict[1], self.value_counts_dict[0] ))\n",
    "                    print ('%s(C%s): imbalance ratio is %.4f'%(self.select_cols[0], class_index, self.imratio ))\n",
    "                print ('-'*30)\n",
    "            else:\n",
    "                print ('-'*30)\n",
    "                imratio_list = []\n",
    "                for class_key, select_col in enumerate(train_cols):\n",
    "                    imratio = self.value_counts_dict[class_key][1]/(self.value_counts_dict[class_key][0]+self.value_counts_dict[class_key][1])\n",
    "                    imratio_list.append(imratio)\n",
    "                    print('Found %s images in total, %s positive images, %s negative images'%(self._num_images, self.value_counts_dict[class_key][1], self.value_counts_dict[class_key][0] ))\n",
    "                    print ('%s(C%s): imbalance ratio is %.4f'%(select_col, class_key, imratio ))\n",
    "                    print ()\n",
    "                self.imratio = np.mean(imratio_list)\n",
    "                self.imratio_list = imratio_list\n",
    "                print ('-'*30)\n",
    "            \n",
    "    @property        \n",
    "    def class_counts(self):\n",
    "        return self.value_counts_dict\n",
    "    \n",
    "    @property\n",
    "    def imbalance_ratio(self):\n",
    "        return self.imratio\n",
    "\n",
    "    @property\n",
    "    def num_classes(self):\n",
    "        return len(self.select_cols)\n",
    "       \n",
    "    @property  \n",
    "    def data_size(self):\n",
    "        return self._num_images \n",
    "    \n",
    "    def image_augmentation(self, image):\n",
    "        img_aug = tfs.Compose([tfs.RandomAffine(degrees=(-15, 15), translate=(0.05, 0.05), scale=(0.95, 1.05), fill=128)]) # pytorch 3.7: fillcolor --> fill\n",
    "        image = img_aug(image)\n",
    "        return image\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self._num_images\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        image = cv2.imread(self._images_list[idx], 0)\n",
    "        image = Image.fromarray(image)\n",
    "        if self.mode == 'train':\n",
    "            image = self.image_augmentation(image)\n",
    "        image = np.array(image)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "        \n",
    "        # resize and normalize; e.g., ToTensor()\n",
    "        image = cv2.resize(image, dsize=(self.image_size, self.image_size), interpolation=cv2.INTER_LINEAR)  \n",
    "        image = image/255.0\n",
    "        __mean__ = np.array([[[0.485, 0.456, 0.406]]])\n",
    "        __std__ =  np.array([[[0.229, 0.224, 0.225]  ]]) \n",
    "        image = (image-__mean__)/__std__\n",
    "        image = image.transpose((2, 0, 1)).astype(np.float32)\n",
    "        if self.class_index != -1: # multi-class mode\n",
    "            label = np.array(self._labels_list[idx]).reshape(-1).astype(np.float32)\n",
    "        else:\n",
    "            label = np.array(self._labels_list[idx]).reshape(-1).astype(np.float32)\n",
    "        return image, label\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    root = '/datasets/chexpert/CheXpert-v1.0-small/'\n",
    "    traindSet = CheXpert(csv_path=root+'train.csv', image_root_path=root, use_upsampling=True, use_frontal=True,\n",
    "     train_cols=['Cardiomegaly', 'Edema', 'Consolidation', 'Atelectasis',  'Pleural Effusion','No Finding'],\n",
    "      image_size=224, mode='train', class_index=-1)\n",
    "    testSet =  CheXpert(csv_path=root+'valid.csv',  image_root_path=root, use_upsampling=False, use_frontal=True, image_size=224, mode='valid', class_index=-1)\n",
    "    trainloader =  torch.utils.data.DataLoader(traindSet, batch_size=32, num_workers=2, drop_last=True, shuffle=True)\n",
    "    testloader =  torch.utils.data.DataLoader(testSet, batch_size=32, num_workers=2, drop_last=False, shuffle=False)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[0.07406456, 0.07406456, 0.07406456, ..., 0.07406456,\n",
       "          0.07406456, 0.07406456],\n",
       "         [0.07406456, 0.07406456, 0.07406456, ..., 0.07406456,\n",
       "          0.07406456, 0.07406456],\n",
       "         [0.07406456, 0.07406456, 0.07406456, ..., 0.07406456,\n",
       "          0.07406456, 0.07406456],\n",
       "         ...,\n",
       "         [0.07406456, 0.07406456, 0.07406456, ..., 0.07406456,\n",
       "          0.07406456, 0.07406456],\n",
       "         [0.07406456, 0.07406456, 0.07406456, ..., 0.07406456,\n",
       "          0.07406456, 0.07406456],\n",
       "         [0.07406456, 0.07406456, 0.07406456, ..., 0.07406456,\n",
       "          0.07406456, 0.07406456]],\n",
       " \n",
       "        [[0.20518208, 0.20518208, 0.20518208, ..., 0.20518208,\n",
       "          0.20518208, 0.20518208],\n",
       "         [0.20518208, 0.20518208, 0.20518208, ..., 0.20518208,\n",
       "          0.20518208, 0.20518208],\n",
       "         [0.20518208, 0.20518208, 0.20518208, ..., 0.20518208,\n",
       "          0.20518208, 0.20518208],\n",
       "         ...,\n",
       "         [0.20518208, 0.20518208, 0.20518208, ..., 0.20518208,\n",
       "          0.20518208, 0.20518208],\n",
       "         [0.20518208, 0.20518208, 0.20518208, ..., 0.20518208,\n",
       "          0.20518208, 0.20518208],\n",
       "         [0.20518208, 0.20518208, 0.20518208, ..., 0.20518208,\n",
       "          0.20518208, 0.20518208]],\n",
       " \n",
       "        [[0.42649236, 0.42649236, 0.42649236, ..., 0.42649236,\n",
       "          0.42649236, 0.42649236],\n",
       "         [0.42649236, 0.42649236, 0.42649236, ..., 0.42649236,\n",
       "          0.42649236, 0.42649236],\n",
       "         [0.42649236, 0.42649236, 0.42649236, ..., 0.42649236,\n",
       "          0.42649236, 0.42649236],\n",
       "         ...,\n",
       "         [0.42649236, 0.42649236, 0.42649236, ..., 0.42649236,\n",
       "          0.42649236, 0.42649236],\n",
       "         [0.42649236, 0.42649236, 0.42649236, ..., 0.42649236,\n",
       "          0.42649236, 0.42649236],\n",
       "         [0.42649236, 0.42649236, 0.42649236, ..., 0.42649236,\n",
       "          0.42649236, 0.42649236]]], dtype=float32),\n",
       " array([0., 0., 0., 0., 0.], dtype=float32))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traindSet[4]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base_model",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
